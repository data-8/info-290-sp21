{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import * # from this library, import all functions\n",
    "from math import *\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Seaborn conveniently has a tool to read in starter datasets \n",
    "cars_df = sns.load_dataset(\"mpg\")\n",
    "cars = Table.from_df(cars_df) # convert from pandas dataframe to datascience table\n",
    "# (more on pandas for the sklearn workshop next week)\n",
    "cars.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the suspicion that the heavier your car is, the longer it takes to get to full speed. In other words, I think that as car weight increases, acceleration decreases. If it does, then by how much?\n",
    "\n",
    "Let's do some linear modeling to figure this out.\n",
    "\n",
    "## Step 0: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: always graph your data!\n",
    "\n",
    "sns.scatterplot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quantify the linearity of this dataset, but notice the scale of the axes! \n",
    "\n",
    "x = cars.column(\"weight\")\n",
    "y = cars.column(\"acceleration\")\n",
    "\n",
    "def standard_units(arr):\n",
    "    return ...\n",
    "\n",
    "weight_su = standard_units(x)\n",
    "accel_su = standard_units(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is a linear transformation; relative distance of points stays the same\n",
    "sns.scatterplot(x = weight_su, y = accel_su);\n",
    "plt.xlabel(\"Weight (standard units)\");\n",
    "plt.ylabel(\"Acceleration (standard units)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's get an actual number. Calculate r using the equation\n",
    "r = ...\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: let's review using scipy.stats\n",
    "from scipy.stats import pearsonr # notice the syntax\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good to me. Now, let's actually build the model!\n",
    "\n",
    "## Step 1: Choose a model\n",
    "\n",
    "In this case, let's focus on 2 models: \n",
    "1. The constant model (guess the same value for every value of x): $$ \\hat{y} = \\theta $$\n",
    "1. A bivariate linear regression model where m = slope and b = intercept: $$ \\hat{y} = mx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The constant model: \n",
    "theta = ...\n",
    "\n",
    "sns.scatterplot(x = \"weight\", y = \"acceleration\", data = cars);\n",
    "plt.axhline(theta, color = \"gold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A linear model; automagically calculated, but we'll do this mathematically on our own\n",
    "\n",
    "sns.regplot(x = \"weight\", y = \"acceleration\", data = cars, ci = None, line_kws = {\"color\": \"gold\"});\n",
    "plt.xlabel(\"weight\");\n",
    "plt.ylabel(\"acceleration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what model is \"better\"? How do we quantify and decide that? \n",
    "\n",
    "## Step 2: Choosing a loss function\n",
    "\n",
    "A loss function is a way we can **quantify how bad a prediction is for a single observation.** If our prediction is close to the true value, loss should be low. If our prediction is far away, then we want a high loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss: mean absolute error\n",
    "def mae(obs, pred):\n",
    "    ...\n",
    "\n",
    "\n",
    "# L2 loss: mean squared error\n",
    "def mse(obs, pred):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit our model by minimizing the objective (L2 loss) function \n",
    "\n",
    "For purposes of our class, we will focus on MSE. Now that we have some models and loss functions, let's work on finding the **inputs** that will reduce our L2 loss.\n",
    "\n",
    "The inputs in this case are as follows:\n",
    "- Constant model = theta\n",
    "- Linear model = slope and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we calculate our model's loss, choose a theta to play with\n",
    "your_theta = ...\n",
    "your_predictions = [your_theta] * cars.num_rows\n",
    "print(\"You choosing \" + str(your_theta)+ \" for theta gives an MSE of\", mse(cars.column(\"acceleration\"), your_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_predictions = np.array([theta] * cars.num_rows)\n",
    "print(\"Using the mean for theta gives an MSE of\", mse(cars.column(\"acceleration\"), constant_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's graph the MSE for a bunch of different thetas (constants)\n",
    "# What is the best?\n",
    "\n",
    "error_thetas = np.linspace(0, 30, 50) # guessing a bunch of thetas from 0-30\n",
    "constant_loss = [mse(cars.column(\"acceleration\"), theta) for theta in error_thetas] \n",
    "# calculate MSE for each individual theta\n",
    "\n",
    "plt.scatter(x = error_thetas, y = constant_loss);  \n",
    "plt.xlabel(\"Theta\");\n",
    "plt.ylabel(\"MSE\");\n",
    "\n",
    "#plt.axvline(theta, color = \"red\"); # something interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property above is why we use **r** to inform our linear regression model; we'll use the mean of the y-values for each particular x-value to predict! \n",
    "\n",
    "So, after some algebraic rearrangements:\n",
    "- y = mx+b\n",
    "\n",
    "In standard units, m = r and b = 0\n",
    "- y_su = r * x + 0\n",
    "\n",
    "In original units, m = r * (SDy/SDx) and b = mean_y - slope * mean_x\n",
    "- y_pred = (r * SDy/SDx) * x + mean_y - (r * SDy/SDx) * mean_x\n",
    "\n",
    "Let's code that in to manually build the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using original units, calculate the descriptive stats\n",
    "r\n",
    "\n",
    "weight_mean = ...\n",
    "weight_std = ...\n",
    "\n",
    "accel_mean = ...\n",
    "accel_std = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = ...\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = ...\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsrl_predictions = ...\n",
    "#lsrl_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing your predictions onto the data!\n",
    "\n",
    "sns.scatterplot(x = \"weight\", y = \"acceleration\", data = cars);\n",
    "plt.scatter(cars.column(\"weight\"), lsrl_predictions, color = \"gold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's your mean squared error for this line?\n",
    "def predict_mse(m, b):\n",
    "    # Using y = mx + b and the cars dataset\n",
    "    ...\n",
    "\n",
    "# How does it compare to the constant mse using the mean of y? (7.585)\n",
    "predict_mse(slope, intercept) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another approach: numerical optimization\n",
    "\n",
    "Using a numerical optimization approach, we can get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted to find the x that gives the minimum value in a parabola, we can use optimization\n",
    "def a_parabola(x):\n",
    "    return (x - 2)**2 + 3\n",
    "\n",
    "parabola_x = np.linspace(-1, 5, 50)\n",
    "plt.scatter(parabola_x, a_parabola(parabola_x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the input that will give the smallest output?\n",
    "minimize(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing for our LSRL using our data and the loss function\n",
    "min_slope, min_intercept = ...\n",
    "\n",
    "print(\"Mathematically calculated slope = \", slope)\n",
    "print(\"minimize slope = \", min_slope)\n",
    "print(\"Mathematically calculated intercept = \", intercept)\n",
    "print(\"intercept = \", min_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellanea: visual diagnostics and non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the \"best\" line, how do we confirm that a line is the best tool?\n",
    "# A residual plot (x on x, residuals on y)\n",
    "# looking for no pattern (random cloud)\n",
    "# equally above and below y = 0 \n",
    "# we don't want to overpredict (negative residual) or underpredict (positive residual)\n",
    "\n",
    "residuals = ...\n",
    "\n",
    "plt.scatter(cars.column(\"weight\"), residuals);\n",
    "plt.axhline(0, color = \"red\");\n",
    "plt.xlabel(\"Weight\");\n",
    "plt.ylabel(\"Residual\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a data transformation\n",
    "# We can use math to make non-linear data linear!\n",
    "\n",
    "sns.scatterplot(x = \"horsepower\", y = \"mpg\", data = cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_x = ...\n",
    "plt.scatter(transformed_x, cars.column(\"mpg\"));\n",
    "plt.xlabel(\"1 / horsepower\");\n",
    "plt.ylabel(\"mpg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and adding a line: using sklearn\n",
    "\n",
    "cars_df[\"1/hp\"] = 1 / cars_df[\"horsepower\"]\n",
    "cars_df = cars_df.dropna()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() # create the model\n",
    "model.fit(cars_df[[\"1/hp\"]], cars_df[\"mpg\"]) # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictions = model.predict(cars_df[[\"1/hp\"]])\n",
    "sklearn_slope = model.coef_ \n",
    "sklearn_intercept = model.intercept_\n",
    "sklearn_slope, sklearn_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(transformed_x, cars.column(\"mpg\"));\n",
    "plt.scatter(cars_df[\"1/hp\"], sklearn_predictions);\n",
    "plt.xlabel(\"1 / horsepower\");\n",
    "plt.ylabel(\"mpg\");\n",
    "\n",
    "# Don't forget to convert when doing predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
