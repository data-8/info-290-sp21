{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "\n",
    "\n",
    "# These lines do some fancy plotting magic.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: a quick intro to pandas\n",
    "\n",
    "Throughout this semester, we've used the `datascience` library: an educational tool that makes using functions from matplotlib, pandas, numpy, scipy, and other libraries easier for beginner Python coders. However, the problem is that, in practice, most data scientists do not use the `datascience` library, instead just using the other libraries directly.\n",
    "\n",
    "In our case, the main tool in industry for **tabular data analysis and manipulations** is `pandas`, which is derived from the term \"panel data\" and \"Python data analysis\". You'll notice that pandas dataframes looks very similar to the `Table` data type we've seen before, but with a few syntax changes, we get a much more powerful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset on car crashes, collected by 538\n",
    "# It's going to be a pandas dataframe; we can read in data with pd.read_csv\n",
    "crashes = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze a dataset, we can do a few things\n",
    "# head will let us view a few rows (like tbl.show())\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with tail\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, how do we access data from this table?\n",
    "# To get a column, we can use bracket notation\n",
    "...\n",
    "# this is called a series; what if we wanted a specific value from this series?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can get multiple columns, but look at the datatype\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what if I wanted to access specific rows/columns from the table? Bracket notation works well for columns, but we can use a better function called loc and iloc to get specific rows/columns.\n",
    "\n",
    "Notice the numbers (0, 1, 2, 3, 4) on the left side of the dataframe. This is called the **index**. This will matter when we use loc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc takes in 2 arguments: rows, columns\n",
    "# Let's find the total crashes where alcohol was involved for index 10 to 20  \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What states do these values correspond to?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a bit annoying to do this, so we can actually just set the states as the index\n",
    "crashes = ...\n",
    "crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I can find specific states using .loc\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I tried using df.iloc?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc works specifically for the index position, so a number\n",
    "...\n",
    "# so even though the index is a string, we can use actual positions instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we know how to work with pandas dataframes, let's do some data manipulations, primarily the equivalent for `tbl.where` and adding/modifying columns.\n",
    "\n",
    "As a note: pandas is capable of **a lot** more features, such as grouping (`df.groupby(\"col\").agg(func)`), pivoting (`df.pivot`), applying (`df.apply(func)`) and joining (`df.merge()`) that are similar to the datascience tools, albeit with more advanced use cases, like outer/inner/left/right joins, for example. \n",
    "\n",
    "Series are also capable of more features than just numpy arrays, including aggregating/counting values (`series.value_counts()`) or finding unique values (`series.unique()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we work with pandas series? Just like arrays!\n",
    "# Let's find the proportion of crashes that involved alcohol\n",
    "alcohol_prop = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to add that to the table, all you need to do is use brackets\n",
    "...\n",
    "crashes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: using sklearn\n",
    "\n",
    "scikit-learn is a machine learning library that offers tools to solve both **supervised learning** problems (regression, classification) and **unsupervised learning** problems (ex. clustering). \n",
    "\n",
    "It **abstracts** away a lot of the mathematics, linear algebra, and code that is involved in creating a proper model. Using pandas, this means that it makes our lives a lot easier when we want to generate models (especially complex, multi-variable models). \n",
    "\n",
    "However, **a word of warning:** just because machine learning makes this process easier, that doesn't mean you shouldn't ignore the data science lifecycle. Always make sure you're using your tools correctly, visualizing your data, cleaning it as necessary, making the correct assumptions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal today: using data about nba players, let's predict the number of points\n",
    "# a player will average, based on their 3 pt field goal attempts\n",
    "\n",
    "nba = ... # import our data: it's called \"nba18-19.csv\"\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if linear regression is a decent tool\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation coefficient\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good! So, now that we have this, we can start using sklearn\n",
    "# First off: let's import our tools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to create a model:\n",
    "# 1) split our data into training and testing sets (using an 80/20 split)\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to create a model:\n",
    "# 1) create a model object\n",
    "our_first_model = ... # we want an intercept!\n",
    "our_first_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) \"train\" the model: the computer will use the \"ordinary least squares\" approach\n",
    "# implemented by scipy.linalg\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) use the model to predict\n",
    "# approach 1: building a line\n",
    "slope, intercept = ... # slope, intercept\n",
    "\n",
    "# If this player attempts 4 3-point attempts, what would be their predicted PTS?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 2: using sklearn predict\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) predicting and evaluating our model (is this a good predictor?)\n",
    "# We will use our /test/ set, since this is supposed to be \"unseen\" data\n",
    "# (remember, we used the /train/ set to build the model - we want to avoid overfitting/bias)\n",
    "\n",
    "# 1st thing to check: r**2 (tldr: what % of data fit the model?)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd thing to check: \"loss\" = mean squared error (how \"off\" are our predictions?)\n",
    "predictions = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing our predictions on the test data!\n",
    "sns.scatterplot(x = \"3PA\", y = \"PTS\", data = nba, alpha = 0.6);\n",
    "plt.scatter(X_test[\"3PA\"], predictions);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plot; another diagnostic\n",
    "residuals = ...\n",
    "\n",
    "# what are we looking for?\n",
    "plt.scatter(X_test[\"3PA\"], residuals);\n",
    "plt.axhline(0, color = \"red\")\n",
    "plt.ylim(-15, 16);\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.xlabel(\"3PA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why is sklearn useful? It makes multi-variate regression (ordinary least squares) really, really easy! Just plug in a dataframe with the variables you want to use to predict when you build the model. \n",
    "\n",
    "In the next cell, we'll use multiple variables (3PA and AST - assists) to predict PTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that this is no longer y = mx + b\n",
    "# This will be: y = m1x1 + m2x2 + b, because we have multiple x-variables\n",
    "\n",
    "# Step 1: train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(nba[[\"3PA\", \"AST\"]], nba[\"PTS\"], \n",
    "                                                    train_size = 0.8, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 and 3: create and fit model\n",
    "multivar_model = LinearRegression(fit_intercept = True) # we want an intercept!\n",
    "multivar_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvslope, mvint = multivar_model.coef_, multivar_model.intercept_\n",
    "mvslope, mvint\n",
    "# Line: y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: predict with the model and evaluating\n",
    "multivar_preds = multivar_model.predict(X_test)\n",
    "multivar_model.score(X_test, y_test) # r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, multivar_preds) # notice that it fits our data better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of mathematical and statistical theory behind these tools, so if you were interested in this content, check out DATA 200 (which focuses a lot on modeling in data science and properly applying these tools to real data).  \n",
    "\n",
    "This lesson was a very quick overview of the field, but hopefully this piques your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
