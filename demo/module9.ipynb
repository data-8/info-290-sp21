{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "from math import *\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing\n",
    "\n",
    "According to Wikipedia, \n",
    ">\"**Anchoring** or focalism is a cognitive bias where an individual depends too heavily on an initial piece of information offered (considered to be the \"anchor\") to make subsequent judgments during decision making. Once the value of this anchor is set, all future negotiations, arguments, estimates, etc. are discussed in relation to the anchor. Information that aligns with the anchor tends to be assimilated toward it, while information that is more dissonant or less related tends to be displaced. This bias occurs when interpreting future information using this anchor to gauge.\"\n",
    "\n",
    "In Psychology 101, Professor Arman Catterson showed his class a survey that asked the students to estimate various values regarding the speed of a cat in mph, the height of a redwood in feet, and the amount of meat Americans eat in pounds. Students in the \"low\" condition were prompted with low values, like 3 mph for cats and 50 lbs for meat, while students in the \"high\" condition were prompted with high values, like 40 mph for cats and 1000 lbs for meat. Did anchoring have an effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchoring = Table().read_table(\"anchoring_data_FA20.csv\")\n",
    "anchoring.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 1 & 2: Choosing hypotheses and a test stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write a set of valid null and alternative hypotheses in this cell.\n",
    "\n",
    "**H0:** ...\n",
    "\n",
    "**Ha:** ...\n",
    "\n",
    "What is a good test statistic?\n",
    "\n",
    "**Type your test stat here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculating the test statistic for the observed data\n",
    "observed_table = ...\n",
    "observed_stat = ...\n",
    "observed_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that calculates the test statistic for any table \n",
    "# in the same format as the anchoring table.\n",
    "\n",
    "def test_statistic(tbl):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "test_statistic(anchoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simulating under the null\n",
    "\n",
    "Recall: if our null hypothesis says that there is no difference between Group A and B, how do we do so in code? We can sample, the same sample size, without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's practice by making 1 table before we use a for loop\n",
    "shuffled_labels = ...\n",
    "one_sim_sample = ...\n",
    "one_sim_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the test stat on one_sim_sample\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works! Let's put this into a for loop. Using iteration,\n",
    "# calculate 1000 test statistics simulated under the null and save it to sim_stats.\n",
    "# It might take a little bit to run, so be patient. \n",
    "\n",
    "sim_stats = ...\n",
    "trials = ...\n",
    "\n",
    "for i in ...:\n",
    "    ...\n",
    "                          \n",
    "sim_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Making a conclusion\n",
    "\n",
    "What direction should we make our p-value calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a histogram of the distribution\n",
    "Table().with_column(\"Simulated Statistics\", sim_stats).hist(bins = np.arange(-20, 20))\n",
    "plt.axvline(observed_stat, color = \"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the p-value, just in case\n",
    "p_value = ...\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the p-value mean? What would we conclude about our hypotheses given this result?\n",
    "\n",
    "**Type your answer here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
