{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import scipy\n",
    "\n",
    "# These lines do some fancy plotting magic. \n",
    "# You can use either seaborn, matplot, or datascience for visualizations.\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding review: pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today, we're going to classify flowers\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Inspect the data\n",
    "# head, tail, describe, shape\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Visualize the data: sepal length vs width - how do we include categories in seaborn?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick way to double check relationships among all variables: sns.pairplot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Create 2 variables: sepal_area and petal_area and add them to the table\n",
    "# for this exercise, let's just assume they're rectangles\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: How do the groups compare in these statistics? Let's group and find the average\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting iris for our classification below \n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Converting to table for datascience library\n",
    "iris_tbl = Table().from_df(iris)\n",
    "iris_tbl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors\n",
    "\n",
    "In this section, we will do the manual approach so we can show the steps of the algorithm.\n",
    "\n",
    "We will stick to only 2 variables to keep this simple (and easy to visualize), but keep in mind -- we could use as many features as we want, as we'll do with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on the pairplot above, what set of features are the \"most\" discriminatory?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we move forward, there's an issue - what's wrong with the ranges of petal_width and petal_length?\n",
    "\n",
    "def standardize(arr):\n",
    "    return ...\n",
    "\n",
    "p_width_norm = standardize(iris_tbl.column(\"petal_width\"))\n",
    "p_length_norm = standardize(iris_tbl.column(\"petal_length\"))\n",
    "\n",
    "iris_tbl_norm = Table().with_columns(\"petal_width norm\", p_width_norm,\n",
    "                                \"petal_length norm\", p_length_norm,\n",
    "                                \"species\", iris_tbl.column(\"species\"))\n",
    "iris_tbl_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split: permute the dataset and then create a separate training and test set by an 80/20 ratio. \n",
    "iris_size = iris_tbl_norm.num_rows\n",
    "num_train_rows = int(iris_size * 0.8)\n",
    "num_test_rows = int(iris_size - num_train_rows)\n",
    "\n",
    "np.random.seed(1234) # Keep everything the same when we re-run the cell\n",
    "iris_shuffled = ...\n",
    "iris_train = ...\n",
    "iris_test = ...\n",
    "\n",
    "iris_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our distance/similarity metric: Euclidean distance\n",
    "# Given 2 arrays, calculate the distance between each point \n",
    "\n",
    "def distance(point1, point2):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first test row has the values: 0.79067, 1.44481\n",
    "# What would we classify it as?\n",
    "first_test_row = make_array(0.79067, 1.44481)\n",
    "distances = make_array()\n",
    "\n",
    "train_features = iris_train.select(0, 1) # we only want the quant data for finding distances\n",
    "\n",
    "for train_row in train_features.rows:\n",
    "    row_array = np.array(list(train_row))\n",
    "    dist_to_unknown = distance(first_test_row, row_array)\n",
    "    distances = np.append(distances, dist_to_unknown)\n",
    "\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the distances with the classes\n",
    "\n",
    "train_with_distance = ...\n",
    "train_with_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest neighbors\n",
    "\n",
    "sorted_train = ...\n",
    "sorted_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the k-nearest neighbors\n",
    "# We'll say k = 5\n",
    "\n",
    "nearest_neighbors = ...\n",
    "nearest_neighbors\n",
    "\n",
    "# The majority of the species column would be our prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were we right?\n",
    "iris_test.take(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the accuracy of our test set, we would use a for loop and iterate through the testing set,\n",
    "# repeating these steps for every single row in the test set to make a prediction for each.\n",
    "# Then, we would compare the predictions with the actual values in the test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN using scikit-learn\n",
    "\n",
    "sklearn abstracts a lot of these steps away! For this, we'll use the same k-value (k = 5), but also we are going to include all features (not just the petal data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataframes to normalize the data\n",
    "iris_df_norm = standardize(iris)\n",
    "iris_df_norm[\"species\"] = iris[\"species\"]\n",
    "iris_df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train-test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(..., ..., train_size = ..., random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "our_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the model\n",
    "# Our first test row:\n",
    "X_test.iloc[0]\n",
    "y_test[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would this model guess? Were we right?\n",
    "our_model.predict(np.array(X_test.iloc[0]).reshape(1, -1)) # puts the example row in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model: how accurate would it be?\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
